%3_bayes.tex
%notes for the course Probability and Statistics COMS10011 
%taught at the University of Bristol
%2018_19 Conor Houghton conor.houghton@bristol.ac.uk

%To the extent possible under law, the author has dedicated all copyright 
%and related and neighboring rights to these notes to the public domain 
%worldwide. These notes are distributed without any warranty. 

\documentclass[11pt,a4paper]{scrartcl}
\typearea{12}
\usepackage{graphicx}
%\usepackage{pstricks}
\usepackage{listings}
\usepackage{color}
\lstset{language=C}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lfoot{\texttt{github.com/COMS10011/2018\_19}}
\lhead{COMS100011 3\_bayes - Conor}
\begin{document}

\section*{Bayes' theorem}

Consider the formula for the conditional probability:
\begin{equation}
P(A|B)=\frac{P(A\cup B)}{P(B)}
\end{equation}
which is equivalent to
\begin{equation}
P(A|B)P(B)=P(A\cup B)
\end{equation}
which tells us that the probability of $A$ and $B$ is the probability
of $B$ multiplied by the probability of $A$ given $B$. This makes lots
of sense, but it is also notable that the left hand side doesn't look
symmetric in $A$ and $B$ while the right hand side clearly is. Obviously this means we can write
\begin{equation}
P(A|B)P(B)=P(B|A)P(A)
\end{equation}
or
\begin{equation}
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\end{equation}
This formula is called the Bayes' rule and is surprisingly useful
because there are lots of interesting problems where we are told $P(B|A)$ but would like to know $P(A|B)$.

Often the example given is related to testing. Lets say 5\% of steaks
sold as beef steak are actually made of horse and imagine we have a
horsiness test which is positive 90\% of the time when tested on horse
and 10\% of the time when tested on beef. If a piece of steak tests
positive for horse, what is the chance it is horse? Let $H$ be the
event of being horse and $Y$ the event of testing positive for
horsiness. Now we know $P(H)=0.05$ and $P(Y|H)=0.9$; what we want is
$P(H|Y)$ and this is what Bayes' rule is useful for:
\begin{equation}
P(H|Y)=\frac{P(Y|H)P(H)}{P(Y)}
\end{equation}

We don't have $P(Y)$ but we can work it out:
\begin{equation}
P(Y)=P(Y|H)P(H)+P(Y|\bar{H})P(\bar{H})
\end{equation}
since $P(Y\cup H)=P(Y|H)P(H)$ and so on. Hence
\begin{equation}
P(Y)=0.9\times 0.05 + 0.1\times 0.95=0.14
\end{equation}
Thus
\begin{equation}
P(H|Y)=\frac{0.9\times 0.05}{0.14}=0.32
\end{equation}
Hence, surprisingly, if a steak tests positive for horsiness it is
still more likely to be beef. Basically, because there are so many
more beef steaks than horse steaks, the relatively small false
positive rate for beef still leads to a reasonably high chance a piece
of steak that tests positive for horse is nonetheless beef.

There is a particular terminology associated with Bayes' rule; it is
sometimes written:
\begin{equation}
\mbox{posterior}=\frac{\mbox{likelihood}\times \mbox{prior}}{\mbox{evidence}}
\end{equation}
The \textsl{posterior} is the probability estimated after the evidence
is gathered, for example, the chance of horsiness after we have found
the test is positive.  The \textsl{likelihood} is how likely the
evidence is given the event, in the example above, it is $P(Y|H)$; the
\textsl{prior} is the probability estimated before the evidence is
gathered, that is $P(H)$, finally \textsl{evidence} measure the
probability of the evidence, $P(Y)$.

\subsection*{Na\"ive Bayes estimator}

Many learning algorithms can be thought of have machines for
estimating probabilities, often in the face of insufficient data to
estimate the probabilities required. A common example used to
illustrate this is a spam filter. Let $W$ represent an ordered list of
words that may be in a email, say:
\begin{equation}
W=(\mbox{enlargement},\mbox{xxx},\mbox{cheapest},\mbox{pharmaceuticals}, \mbox{satisfied},\mbox{leeds})
\end{equation}
and say $\textbf{w}$ is a vector of zeros and ones indicating the
presence or absense of different potential spam words in an email. Thus, an email that includes the words \lq{}enlargment\lq{}, \lq{}xxx\rq{} and \lq{}leeds\rq{} but not \lq{}cheapest\rq{}, \lq{}pharmaceuticals\rq{} and \lq{}satisfied\rq{} would be represented by
\begin{equation}
\textbf{w}=(1,1,0,0,0,1)
\end{equation}
Now let $S$ represent the event of an email being spam. The objective
with a spam filter is to estimate $P(S|\textbf{w})$ for every possible
vector $\textbf{w}$ and then use a cut-off to label any email with a
high probability of being spam as \lq{}spam\rq{}.

Obviously if you have a truely huge amount of data you could estimate this probability by counting:
\begin{equation}
P(S|(1,1,0,0,0,1))=\frac{\#\{\mbox{spam emails with the words enlargement, xxx and leeds}\}}{\#\{\mbox{all emails with the words enlargement, xxx and leeds}\}}
\end{equation}
However there are $2^6=64$ possible $\textbf{w}$ vectors, and of
course in a real example you'd need many more than six words, thus,
for anything but an unfeasibly large data set, the amount of emails
with the precise combination of words represented by a given
$\textbf{w}$ will be tiny, leading to a poor estimate of the
probabilities.

An alternative approach is to use Bayes's rule to get
\begin{equation}
P(S|\textbf{w})=\frac{P(\textbf{w}|S)P(S)}{P(\textbf{w})}
\end{equation}
This doesn't look any better, $P(\textbf{w}|S)$ is no easier to
estimate than $P(S|\textbf{w})$. However, in the na\"{i}ve Bayes
estimator it is additionally assumed that the different words are
independent so that 
\begin{eqnarray}
P((1,1,0,0,0,1)|S)&=&P(\mbox{enlargement}|S)P(\mbox{xxx}|S)[1-P(\mbox{cheapest}|S)]\times\cr&&[1-P(\mbox{pharmaceuticals}|S)][1-P(\mbox{satisfied}|S)]P(\mbox{leeds}|S)
\end{eqnarray}
This is clearly inaccurate, a spam email is \lq{}enlargement\rq{} is
more likely to contain \lq{}satisfied\rq{} than one that doesn't, that
is why it is a \lq{}na\"ive\rq{} classifier. The advantage though is
that the individual probabilities are much easier to estimate, there
will be more emails with \lq{}leeds\rq{} in that there will be emails
with the exact combination of words represented by $(1,1,0,0,0,1)$
and so counting occurances will be much more accurate.


\end{document}

